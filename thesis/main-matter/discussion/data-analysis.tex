\section{Analiza danych}
\label{sec:data-analysis}

Alfabet Polskiego Języka Migowego można podzielić na dwie kategorie: litery statyczne i dynamiczne. W kategorii dynamicznych liter znajdują się wszystkie litery ze znakami diakrytycznymi oraz dodatkowo litery $D$, $F$, $G$, $H$, $J$ i $Z$. Istnieją także litery specjalne takie jak $CH$, $CZ$, $RZ$ oraz $SZ$, które również wchodzą w skład alfabetu i składają się z sekwencji ruchów. Zdecydowana większość liter w PJM jest dynamiczna, co oznacza, że są wykonywane poprzez ruchy dłoni. Mniejszość liter jest statyczna, więc w prezentowanym zbiorze danych te znaki pokazywane są w bezruchu. Nawet większość publicznie dostępnych zbiorów danych alfabetu ASL składających się z ogromnej ilości zdjęć, nie zawiera dynamicznych znaków, ze względu na to, że są to zdjęcia.

Warto podkreślić, że brak ogólnodostępnych danych zawierających dynamiczne litery był głównym motywem stworzenia zbioru danych. Na szczęście utworzony zbiór danych jest bogatszy o dynamiczne znaki, co czyni go kompletnym i jest to przede wszystkim przewaga nad istniejącymi rozwiązaniami, która odpowiada rzeczywistym potrzebom. Od samego początku była pełna kontrola nad ilością danych, a zatem zbiór jest zrównoważony. Jednym z niewielkich ograniczeń końcowego rozwiązania był fakt, że długość każdego znaku jest stała. Wszystkie litery, niezależnie od złożoności sekwencji znaku trwają tyle samo na nagraniach.

W przypadku projektu nie był to poważny problem, głównie dlatego, że daktylografia Polskiego Języka Migowego nie składa się ze skomplikowanych i długich sekwencji znaków. Długość próbki nagrań każdego znaku została odpowiednio dostosowana, aby mieściła się w czasie zarówno podczas nagrywania, jak i podczas procesu klasyfikacji sekwencji współrzędnych. Prędkość zmniejszonych klatek została ustalona na podstawie średniej ilości klatek na sekundę z pomiaru przeprowadzonego zarówno na urządzeniach mobilnych, jak i w przeglądarkach na urządzeniach stacjonarnych, co pozwoliło na skuteczne przetwarzanie i analizę znaków. Oprócz regularnej klasyfikacji sekwencji punktów z położeniem dłoni, należy pamiętać, że każda klatka musiała być również przetworzona, aby określić położenie dłoni przed kamerą. W tym kontekście, JavaScript był najbardziej odpowiednim rozwiązaniem do projektowania aplikacji przeglądarkowych. Mimo że Python jest użytecznym językiem programowania, ma znacznie niższą wydajność niż JavaScript, zwłaszcza w przypadku wykonywania skryptów po stronie klienta~\cite{lei2014}.

Dane tabelaryczne powstały z potrzeby usunięcia niepotrzebnych zmiennych i zmniejszenia rozmiaru danych. W związku z tym, zmniejszyła się również złożoność i wymagania obliczeniowe. W drugiej iteracji projektu, dane pochodziły z estymacji położenia rąk za pomocą aktywnie wspieranej biblioteki MediaPipe, która umożliwiła uzyskanie współrzędnych z przetworzonych filmów. Jednym z głównych atutów tego podejścia była gotowa infrastruktura, która pozwoliła uniknąć implementacji algorytmu do ekstrakcji danych od zera, więc w dalszym etapie pracy zadaniem było tylko zbudowanie aplikacji oraz wyuczenie modelu, który klasyfikuje sekwencje koordynatów. Korzystając z GPU na platformie Google Colab, udało się znacznie przyspieszyć proces trenowania modelu bez potrzeby konfiguracji środowiska.

Wybór sieci LSTM w projekcie nie był przypadkowy. Kluczowe było uniknięcie problemu znikającego gradientu, który może wystąpić w sieciach rekurencyjnych. Zbiór danych był dość obszerny, a zadaniem modelu było klasyfikowanie prawdopodobieństw dla 36 różnych znaków. W początkowym etapie eksperymentów dobrano niewielkie wartości parametrów, aby monitorować zachowanie modelu w trakcie procesu uczenia. Otrzymane wyniki były obiecujące, biorąc pod uwagę brak zaawansowanych technik optymalizacyjnych poza algorytmem Adam~\cite{kingma2015} w domyślnej konfiguracji. Chociaż Adam dostosowuje tempo uczenia adaptacyjnie, to nadal istniała w późńiejszym etapie potrzeba początkowego określenia stałej uczenia, aby algorytm miał punkt odniesienia.

Proces trenowania modelu okazał się niestabilny, co zostało odzwierciedlone na wykresach oraz w macierzy pomyłek. Model miał trudności w rozróżnianiu znaków o podobnych układach palców, co było najbardziej zauważalne zwłaszcza w przypadku liter diakrytyzowanych, które różnią się głównie ruchem i zmiennym kątem nachylenia dłoni. W drugiej iteracji nie chciano manualnie szukać optymalnych parametrów metodą prób i błędów, dlatego skorzystano z algorytmu \mintinline{python}{Hyperband} do szukania hiperparametrów lepszego modelu. W implementacji było 216 kombinacji parametrów, więc manualne szukanie byłoby kosztowne czasowo oraz obliczeniowo.

\mintinline{python}{Hyperband} to algorytm optymalizacji hiperparametrów, który wykorzystuje technikę randomizowanego przeszukiwania oraz sukcesywnego zwiększania zasobów dostępnych dla najlepszych konfiguracji. Działa na zasadzie iteracyjnego trenowania i porównywania modeli, eliminując te, które osiągają słabe wyniki, alokując więcej zasobów dla obiecujących konfiguracji~\cite{li2017}. W celu zoptymalizowania procesu uczenia modelu, wprowadzono pewne modyfikacje po warstwie \mintinline{python}{LSTM}. Konkretnie, dodano warstwę porzucenia~\cite{hinton2012} oraz zastosowano normalizację danych \mintinline{python}{BatchNormalization}. W trakcie eksperymentów dostosowano trzy kluczowe parametry:

\begin{enumerate}
    \item Liczbę jednostek w warstwie \mintinline{python}{LSTM}
    \item Prawdopodobieństwo porzucenia neuronów w warstwie \mintinline{python}{Dropout}
    \item Stałą uczenia w algorytmie optymalizacyjnym \mintinline{python}{Adam}
\end{enumerate}

Aby zapobiec zjawisku przetrenowania i jednocześnie przyspieszyć proces trenowania modeli, wdrożono mechanizm callbacku \mintinline{python}{EarlyStopping} podczas przeszukiwania. Dzięki temu nie musiano stale monitorować spadku wartości funkcji straty podczas trenowania, ponieważ model samoczynnie zatrzymywał proces uczenia, gdy nie zanotowano już poprawy w wynikach. To pozwoliło oszczędzić czas i zabezpieczyć się przed przetrenowaniem modelu.

Dropout działa na zasadzie losowego wyłączania pewnych neuronów lub jednostek w trakcie trenowania. Polega to na tym, że podczas każdej iteracji procesu uczenia, warstwa porzucenia losowo dezaktywuje pewien procent neuronów w danej warstwie. Te dezaktywowane neurony nie biorą udziału w obliczeniach ani w propagacji wstecznej błędów, co prowadzi do pewnego rodzaju usunięcia części informacji z modelu w trakcie treningu. Dzięki temu, model musi polegać na różnych neuronach w różnych iteracjach, co pomaga w zwiększeniu zdolności generalizacji.

Normalizacja podzbioru danych (ang. batch) treningowych polega na obliczeniu średniej i odchylenia standardowego dla każdej cechy w danym podzbiorze danych treningowych i następnie przekształceniu danych w ten sposób, aby miały średnią bliską zeru i odchylenie standardowe bliskie jedności. To przekształcenie pozwala na lepszą stabilność procesu uczenia poprzez utrzymanie wartości na wyjściach warstw w podobnym zakresie. Normalizacja batchowa działa jako rodzaj regularyzacji, co może pomóc w zapobieganiu przeuczenia. Normalizacja batchowa pozwoliła również na używanie większych wartości współczynnika uczenia, więc najprawdopodobniej w tabeli najlepszych parametrów można by było dostrzec inne parametry~\cite{ioffe2015}. Wśród szukanych wartości $0.001$ okazało się być złotym środkiem.

Mając wytrenowany model, który radził sobie lepiej niż wcześniej, zdecydowano się sprawdzić, czy można jeszcze bardziej polepszyć wyniki. W 39 epoce hipermodel osiągnął najlepszą wartość funkcji straty, więc postanowiono trenować model na tych samych parametrach, ale wyłącznie w tej architekturze warstw, którą stosowano wcześniej. Po 35 epoce model uzyskał jeszcze lepszy wynik, ale niestety proces trenowania był niestabilny, a wyniki na zbiorze treningowym i walidacyjnym przestały się dobrze pokrywać. Ostatecznie, wynik końcowy okazał się być gorszy niż w przypadku poprzedniego modelu, który trenowaliśmy do 50 epoki. Z tego powodu, w finalnym rozwiązaniu przywrócono wagi pierwszego hipermodelu, z racji, że dawał on lepsze wyniki.

\begin{figure}[b]
    \centering
    \begin{subfigure}[H]{0.3\textwidth}
        \centering
        \includesvg[width=\textwidth]{figures/tensorboard}
        \caption{TensorBoard}
        \label{fig:tensorboard}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.3\textwidth}
        \centering
        \includesvg[width=\textwidth]{figures/application}
        \caption{Aplikacja webowa}
        \label{fig:web-application}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.3\textwidth}
        \centering
        \includesvg[width=\textwidth]{figures/github}
        \caption{GitHub}
        \label{fig:github}
    \end{subfigure}
    \caption{Kody QR}
    \label{fig:qr-codes}
\end{figure}
