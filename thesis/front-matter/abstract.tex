\hypersetup{pageanchor=false}

\begin{abstract}
W przypadku języka mówionego, alfabet stanowi podstawę komunikacji, umożliwiając tworzenie wyrazów i zrozumienie skomplikowanych zdań. Jednak w przypadku języka migowego, sytuacja jest zupełnie inna. Daktylografia migowa odgrywa kluczową rolę jako most łączący dwa odmienne języki pod względem struktury: język migowy i język fonetyczny. Poszczególne znaki migowe reprezentują litery języka fonetycznego, ale same w sobie nie tworzą słów. Zamiast tego, stanowią one niezastąpiony element przekazywania znaczenia i komunikacji w języku migowym. Omówiona praca koncentrowała się na stworzeniu modelu klasyfikacji, który mógł obsługiwać zarówno statyczne, jak i dynamiczne litery Polskiego Języka Migowego (PJM), co stanowiło wyzwanie w kontekście braku ogólnodostępnych zbiorów danych. Pierwszym krokiem było zebranie własnych danych, ponieważ dostępne zbiory nie uwzględniały znaków dynamicznych, które są kluczowym elementem PJM. Udokumentowany zbiór danych obejmował 36000 filmów dwusekundowych, co pozwoliło stworzyć kompleksowy i zrównoważony zbiór danych do projektu. W trakcie procesu uczenia modelu wykorzystano algorytm Adam do optymalizacji hiperparametrów, a także zastosowano techniki, takie jak dropout i normalizacja danych, aby zwiększyć wydajność modelu. Badania wykazały, że istnieje duża potrzeba dalszego rozwoju w dziedzinie przetwarzania języka migowego. Istniejące modele i zbiory danych często pomijają dynamiczne znaki, a także nie uwzględniają innych aspektów języka migowego, takich jak mimika czy długość trwania znaków. Niniejsza praca stanowi krok w kierunku zrozumienia i rozwinięcia tej unikalnej dziedziny lingwistyki migowej, otwierając nowe możliwości komunikacji między światem Głuchych a słyszących.
    \vfill
    \section*{Słowa kluczowe}
    \noindent{język migowy, detekcja gestów, dostępność cyfrowa, estymacja pozy, klasyfikacja akcji}
    \thispagestyle{empty}
\end{abstract}

\begin{otherlanguage}{english}
    \begin{abstract}
In the case of spoken language, the alphabet serves as the foundation of communication, enabling the creation of words and the understanding of complex sentences. However, in the case of sign language, the situation is entirely different. Sign language dactylology plays a crucial role as a bridge connecting two distinct languages in terms of structure: sign language and the phonetic language. Individual sign symbols represent letters from the phonetic language, but they do not form words by themselves. Instead, they constitute an indispensable element for conveying meaning and communication in sign language. The discussed work focused on creating a classification model capable of handling both static and dynamic letters of Polish Sign Language (PJM), which posed a challenge due to the lack of publicly available datasets. The first step involved collecting our own data, as existing datasets did not include dynamic signs, which are a crucial element of PJM. The documented dataset encompassed 36000 two-second videos, enabling the creation of a comprehensive and balanced dataset for the project. During the model training process, the Adam algorithm was used for hyperparameter optimization, and techniques such as dropout and data normalization were applied to enhance model performance. Research has shown that there is a significant need for further development in the field of sign language processing. Existing models and datasets often overlook dynamic signs and do not consider other aspects of sign language, such as facial expressions or sign duration. This work represents a step towards understanding and advancing this unique field of sign linguistics, opening new possibilities for communication between the Deaf and hearing worlds.
        \vfill
        \section*{Keywords}
        \noindent{sign language, gesture recognition, digital accessibility, pose estimation, action classification}
        \thispagestyle{empty}
    \end{abstract}
\end{otherlanguage}
