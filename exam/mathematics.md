# MATEMATYKA

## CaÅ‚ka nieoznaczona, oznaczona, zastosowanie i techniki obliczania

CaÅ‚ka jest podstawowym pojÄ™ciem w matematyce, ktÃ³re jest zwiÄ…zane z obliczaniem obszarÃ³w pod krzywymi oraz odwrotnym procesem rÃ³Å¼niczkowania.

### CaÅ‚ka nieoznaczona

To odwrÃ³cona operacja kalkulacji pochodnej. Kiedy oblicza siÄ™ pochodnÄ…, znajduje siÄ™ tempo zmiany funkcji. CaÅ‚ka nieoznaczona jest odwrotnoÅ›ciÄ… tego procesu. Oznacza to, Å¼e dla danej funkcji $f$ istnieje $F$, ktÃ³rej pochodna $F'$ jest rÃ³wna $f$, zatem $F'(x) = f(x)$.

FunkcjÄ™ $F(x)$ speÅ‚niajÄ…cÄ… powyÅ¼szy warunek nazywa siÄ™ funkcjÄ… pierwotnÄ….

> **WzÃ³r caÅ‚ki nieoznaczonej**
>
> $\begin{aligned}\int f(x) \, dx = F(x) + C\end{aligned}$
>
> ObecnoÅ›Ä‡ staÅ‚ej caÅ‚kowania $C$ wynika z faktu, Å¼e pochodna staÅ‚ej jest zawsze rÃ³wna zeru.

### CaÅ‚ka oznaczona

CaÅ‚ka oznaczona jest rÃ³wna polu powierzchni pod krzywÄ… funkcji $f(x)$ w okreÅ›lonym przedziale. JeÅ¼eli $F(x)$ jest funkcjÄ… pierwotnÄ… funkcji $f(x)$ ciÄ…gÅ‚ej w danym przedziale $\left\langle{x_1};{x_2}\right\rangle$, to rÃ³Å¼nicÄ™ funkcji pierwotnych $F(x_1)$ oraz $F(x_2)$ nazywamy caÅ‚kÄ… oznaczonÄ… dla funkcji $f$ od $x_1$ do $x_2$.

> **WzÃ³r caÅ‚ki nieoznaczonej**
>
> $\begin{aligned}\int_{x_1}^{x_2}{f(x) \, dx} = \left[F(x)\right]_{x_1}^{x_2} = {F(x_2) - F(x_1)}\end{aligned}$

### Techniki obliczania caÅ‚ek

-   **CaÅ‚kowanie funkcji elementarnych**

    Obliczanie caÅ‚ek funkcji podstawowych na podstawie reguÅ‚ rÃ³Å¼niczkowania i wiedzy o pochodnych. To po prostu znalezienie funkcji, ktÃ³rej pochodna jest zadanÄ… funkcjÄ…. Na przykÅ‚ad, jeÅ›li pochodna funkcji to $2x$, to caÅ‚ka nieoznaczona tej funkcji to $x^2$.

-   **CaÅ‚kowanie metodÄ… podstawienia**

    CaÅ‚kowanie przez podstawienie stosujemy, gdy wÅ›rÃ³d funkcji podcaÅ‚kowej potrafimy wyodrÄ™bniÄ‡ pewnÄ… funkcjÄ™ i jej pochodnÄ…. Polega na zamianie zmiennych w celu uproszczenia caÅ‚ki.

> **PrzykÅ‚adowe obliczenie caÅ‚ki $\int{8\cos{4x} \, dx}$**
>
> 1.  Podstawienie $4x$ jako nowej zmiennej $t$ i obliczenie pochodnej $t$ wzglÄ™dem $x$
>
>     $
>     \begin{aligned}
>         t & = 4x & dt & = 4 \, dx \\
>         \frac{dt}{dx} & = 4 & dx & = \frac{dt}{4}
>     \end{aligned}
>     $
>
> 2.  Podstawienie zmiennych do wyraÅ¼enia
>
>     $\begin{aligned}\int{8\cos{t} \, \frac{dt}{4}} = \int{\cancel{8}\cos{t} \, \frac{dt}{\cancel{4}}} = \int{2\cos{t} \, dt}\end{aligned}$
>
> 3.  Uproszczenie wyraÅ¼enia wzorem $\int{\cos{x} \, dx} = \sin{x} + C$
>
>     $\begin{aligned}\int{2\cos{t} \, dt} = 2\sin{t} + C\end{aligned}$
>
> 4.  Podstawienie pierwotnej wartoÅ›ci do zmiennej $t$
>
>     $\begin{aligned}\int{8\cos{4x} \, dx} = 2\sin{t} + C = 2\sin{4x} + C\end{aligned}$

-   **CaÅ‚kowanie przez czÄ™Å›ci**

    Wykorzystuje siÄ™ do rozkÅ‚adania caÅ‚ki dwÃ³ch funkcji, korzystajÄ…c z reguÅ‚y pochodzenia iloczynu dwÃ³ch funkcji. CaÅ‚kowanie przez czÄ™Å›ci jest dozwolone wtedy i tylko wtedy, gdy pochodne obu funkcji sÄ… funkcjami ciÄ…gÅ‚ymi.

    JeÅ¼eli $f$ i $g$ majÄ… ciÄ…gÅ‚e pochodne, to $\int{f(x) \cdot g'(x) \, dx} = f(x) \cdot g(x) âˆ’ \int{f'(x) \cdot g(x) \, dx}$.

> **PrzykÅ‚adowe obliczenie caÅ‚ki $\int{x \sin{x} \, dx}$**
>
> 1.  Obliczenie pochodnych funkcji $f$ i $g$
>
>     $
>     \begin{aligned}
>         f(x) & = x & g'(x) & = \sin{x} \\
>         f'(x) & = 1 & g(x) & = \cos{x}
>     \end{aligned}
>     $
>
> 2.  Podstawienie do wzoru
>
>     $\begin{aligned}\int{x\sin{x} \, dx} = x\cos{x} - \int{1 \cdot \cos{x} \, dx}\end{aligned}$
>
> 3.  Uproszczenie wyraÅ¼enia
>
>     $\begin{aligned}x\cos{x} - \int{\cos{x} \, dx} = x\cos{x} - \sin{x} + C\end{aligned}$

-   **Obliczanie pola pod krzywÄ… dla przedziaÅ‚u**

> **PrzykÅ‚adowe obliczenie pola pod funkcjÄ… $3x^2 + 5$ w przedziale $\left\langle3;5\right\rangle$**
>
> 1.  Obliczenie caÅ‚ki $\int{3x^2 + 5 \, dx}$
>
>     $\begin{aligned}\int{3x^2 + 5 \, dx} = x^3 + 5x\end{aligned}$
>
> 2.  Podstawienie do wzoru
>
>     $
>     \begin{aligned}
>         \int_{3}^{5}{3x^2 + 5 \, dx} & = \left[x^3 + 5x\right]_3^5 \\
>         \int_{3}^{5}{3x^2 + 5 \, dx} & = (5^3 + 5 \cdot 5) - (3^3 + 5 \cdot 3) = (125 + 25) - (27 + 15) \\
>         \int_{3}^{5}{3x^2 + 5 \, dx} & = 150 - 42 = 108
>     \end{aligned}
>     $

## Wielomian i szereg Taylora funkcji rzeczywistej

JeÅ¼eli funkcja $f$ ma w przedziale $\left\langle x_0; x \right\rangle$ pochodnÄ… rzÄ™du $n - 1$ oraz pochodnÄ… rzÄ™du $n$ w przedziale $(x_0; x)$, wÃ³wczas istnieje takie $c$, Å¼e $c \in (x_0; x)$.

> **WzÃ³r wielomianu Taylora**
>
> $\begin{aligned}f(x) = f(x_0) + \frac{f^{(1)}(x_0)}{1!} \cdot (x - x_0) + \ldots + \frac{f^{(n-1)}(x_0)}{(n-1)!} \cdot (x - x_0)^{n - 1} + \frac{f^{(n)}(c)}{n!} \cdot (x - x_0)^n\end{aligned}$
>
> **Wielomian Taylora**
>
> $\begin{aligned}f(x_0) + \frac{f^{(1)}(x_0)}{1!} \cdot (x - x_0) + \ldots + \frac{f^{(n-1)}(x_0)}{(n-1)!} \cdot (x - x_0)^{n - 1}\end{aligned}$
>
> **n-ta reszta Lagrange'a rozwiniÄ™cia Taylora**
>
> $\begin{aligned}R_n = \frac{f^{(n)}(c)}{n!} \cdot (x - x_0)^n\end{aligned}$

### Szereg Taylora

Jest to matematyczny sposÃ³b przybliÅ¼ania funkcji rÃ³Å¼niczkowalnej za pomocÄ… nieskoÅ„czonego szeregu wielomianowego. Dla funkcji nieskoÅ„czenie wiele razy rÃ³Å¼niczkowalnych uzasadnione jest rozwaÅ¼anie nieskoÅ„czonych wielomianÃ³w Taylora.

> **WzÃ³r szeregu Taylora**
>
> $\begin{aligned}\sum_{n = 0}^{\infty}{\frac{f^{(n)}(x_0)}{n!} \cdot (x - x_0)^n}\end{aligned}$

Szeregiem Maclaurina nazywamy szereg Taylora, dla ktÃ³rego $x_0 = 0$.

> **WzÃ³r szeregu Maclaurina**
>
> $\begin{aligned}\sum_{n = 0}^{\infty}{\frac{f^{(n)}(0)}{n!} \cdot x^n}\end{aligned}$

## UkÅ‚ady rÃ³wnaÅ„ liniowych, rÃ³Å¼ne metody rozwiazywania, liczba rozwiÄ…zaÅ„

UkÅ‚adem $m$ rÃ³wnaÅ„ liniowych z $n$ niewiadomymi $x_1$, $x_2$, ..., $x_n$, gdzie $m$, $n \in \N$ nazywamy ukÅ‚ad rÃ³wnaÅ„, gdzie $a_{ij} \in \R$, $b_i \in \R$ dla $1 \leq i \leq m$ oraz $1 \leq n \leq n$.

RozwiÄ…zaniem ukÅ‚adu rÃ³wnaÅ„ liniowych nazywamy ciÄ…g ($x_1$, $x_2$, $\ldots$, $x_n$) liczb rzeczywistych speÅ‚niajÄ…cych ten ukÅ‚ad. UkÅ‚ad rÃ³wnaÅ„, ktÃ³ry nie ma rozwiÄ…zania, nazywamy ukÅ‚adem sprzecznym.

> **PostaÄ‡ ukÅ‚adu rÃ³wnaÅ„**
>
> $
> \begin{aligned}
>     \begin{cases}
>         a_{11} x_1 & + & a_{12} x_2 & + & \ldots & + & a_{1n} x_n & = b_1 \\
>         a_{21} x_1 & + & a_{22} x_2 & + & \ldots & + & a_{2n} x_n & = b_2 \\
>         \vdots & & \vdots & & \ddots & & \vdots & \vdots \\
>         a_{m1} x_1 & + & a_{m2} x_2 & + & \ldots & + & a_{mn} x_n & = b_m
>     \end{cases}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     \begin{bmatrix}
>         a_{11} x_1 & + & a_{12} x_2 & + & \ldots & + & a_{1n} x_n \\
>         a_{21} x_1 & + & a_{22} x_2 & + & \ldots & + & a_{2n} x_n \\
>         \vdots & & \vdots & & \ddots & & \vdots \\
>         a_{m1} x_1 & + & a_{m2} x_2 & + & \ldots & + & a_{mn} x_n
>     \end{bmatrix}
>     =
>     \begin{bmatrix}
>         b_1 \\
>         b_2 \\
>         \vdots \\
>         b_m
>     \end{bmatrix}
>     \begin{bmatrix}
>         a_{11} & a_{12} & \ldots & a_{1n} \\
>         a_{21} & a_{22} & \ldots & a_{2n} \\
>         \vdots & \vdots & \ddots & \vdots \\
>         a_{m1} & a_{m2} & \ldots & a_{mn}
>     \end{bmatrix}
>     \begin{bmatrix}
>         x_1 \\
>         x_2 \\
>         \vdots \\
>         x_n
>     \end{bmatrix}
>     =
>     \begin{bmatrix}
>         b_1 \\
>         b_2 \\
>         \vdots \\
>         b_m
>     \end{bmatrix}
> \end{aligned}
> $

### Metody rozwiÄ…zywania

#### Metoda substytucji

Ta metoda jest najprostsza i czÄ™sto stosowana do ukÅ‚adÃ³w rÃ³wnaÅ„ liniowych. Polega na rozwiÄ…zaniu jednego rÃ³wnania wzglÄ™dem jednej zmiennej i podstawieniu tego wyniku do pozostaÅ‚ych rÃ³wnaÅ„. NastÄ™pnie powtarza siÄ™ ten proces dla pozostaÅ‚ych zmiennych, aÅ¼ do uzyskania rozwiÄ…zania.

#### Metoda eliminacji Gaussa

Jest to bardziej ogÃ³lna metoda rozwiÄ…zywania ukÅ‚adÃ³w rÃ³wnaÅ„ liniowych. Polega na przeksztaÅ‚caniu ukÅ‚adu rÃ³wnaÅ„ tak, aby stopniowo eliminowaÄ‡ zmienne i uzyskaÄ‡ wyniki. ObliczajÄ…c rzÄ…d macierzy metodÄ… Gaussa, naleÅ¼y za pomocÄ… operacji elementarnych na wierszach sprowadziÄ‡ macierz do macierzy schodkowej. Wtedy wszystkie niezerowe wiersze sÄ… liniowo niezaleÅ¼ne i moÅ¼na Å‚atwo odczytaÄ‡ rzÄ…d macierzy.

> **PrzykÅ‚adowe rozwiÄ…zanie metodÄ… Gaussa**
>
> $
> A \mid B =
> \begin{cases}
>     x_1 & - & 3x_2 & + & 2x_3 & = & 3 \\
>     x_1 & + & x_2 & - & 2x_3 & = & 1 \\
>     -2x_1 & - & x_2 & + & x_3 & = & -1
> \end{cases}
> $
>
> $
> A \mid B =
> \left[
>     \begin{matrix}
>         1 & -3 & 2 \\
>         1 & 1 & -2 \\
>         2 & -1 & 1
>     \end{matrix}
>     \, \left |\,
>     \begin{matrix}
>         3 \\
>         1 \\
>         -1
>     \end{matrix}
>     \right.
> \right]
> $
>
> 1. Odejmowanie wiersza pierwszego od wiersza drugiego i dwukrotnie od wiersza trzeciego
>
>     $
>     A \mid B \sim
>     \left[
>         \begin{matrix}
>             1 & -3 & 2 \\
>             0 & 4 & -4 \\
>             0 & 5 & -3
>         \end{matrix}
>         \, \left |\,
>         \begin{matrix}
>             3 \\
>             -2 \\
>             -7
>         \end{matrix}
>         \right.
>     \right]
>     $
>
> 2. Dzielenie drugiego wiersza przez 4
>
>     $
>     A \mid B \sim
>     \left[
>         \begin{matrix}
>             1 & -3 & 2 \\
>             0 & 1 & -1 \\
>             0 & 5 & -3
>         \end{matrix}
>         \, \left |\,
>         \begin{matrix}
>             3 \\
>             -0.5 \\
>             -7
>         \end{matrix}
>         \right.
>     \right]
>     $
>
> 3. Odejmowanie piÄ™ciokrotnie wiersza drugiego od wiersza trzeciego
>
>     $
>     A \mid B \sim
>     \left[
>         \begin{matrix}
>             1 & -3 & 2 \\
>             0 & 1 & -1 \\
>             0 & 0 & 2
>         \end{matrix}
>         \, \left |\,
>         \begin{matrix}
>             3 \\
>             -0.5 \\
>             -4.5
>         \end{matrix}
>         \right.
>     \right]
>     $
>
> 4. Dzielenie trzeciego wiersza przez 2 i doprowadzenie do macierzy schodkowej
>
>     $
>     A \mid B \sim
>     \left[
>         \begin{matrix}
>             1 & -3 & 2 \\
>             0 & 1 & -1 \\
>             0 & 0 & 1
>         \end{matrix}
>         \, \left |\,
>         \begin{matrix}
>             3 \\
>             -0.5 \\
>             -2.25
>         \end{matrix}
>         \right.
>     \right]
>     $
>
>     $
>     A \mid B \sim
>     \begin{cases}
>         x_1 & - & 3x_2 & + & 2x_3 & = & 3 \\
>         & & x_2 & - &  x_3 & = & -0.5 \\
>         & & & & x_3 & = & -2.25
>     \end{cases}
>     $
>
> 5. Obliczenie niewiadomych
>
>     $
>     x_3 = -2.25
>     $
>
>     $
>     \begin{aligned}
>         x_2 & - x_3 = -0.5  \\
>         x_2 & = -0.5 + x_3  \\
>         x_2 & = -0.5 - 2.25 \\
>         x_2 & = -2.75 \\
>     \end{aligned}
>     $
>
>     $
>     \begin{aligned}
>         x_1 & - 3x_2 + 2x_3 = 3 \\
>         x_1 & = 3 + 3x_2 - 2x_3 \\
>         x_1 & = 3 + 3 \cdot (-2.75) - 2 \cdot (-2.25) \\
>         x_1 & = 3 - 8.25 + 4.5 \\
>         x_1 & = -0.75
>     \end{aligned}
>     $

#### Metoda Cramera

UkÅ‚adem Cramera jest jest ukÅ‚ad oznaczony (ma dokÅ‚adnie jedno rozwiÄ…zanie) wtedy i tylko wtedy, gdy ma on niezerowy wyznacznik. Liczba rÃ³wnaÅ„ ukÅ‚adu Cramera musi byÄ‡ rÃ³wna liczbie jego nie wiadomych.

Niech dany bÄ™dzie ukÅ‚ad rÃ³wnaÅ„ liniowych $x_1 \cdot a_1 + \ldots + x_n \cdot a_n = b$, gdzie $x = (x_1, \ldots, x_n)$ oraz $b = (b_1, \ldots, b_n)$.

1. $\det(a_1, \ldots, a_n) \ne 0$

-   **ukÅ‚ad oznaczony** (ma jedno i tylko jedno rozwiÄ…zanie)

> **Wzory oznaczonego ukÅ‚adu Cramera**
>
> $
> \begin{aligned}
>     x_1 & = \frac{\det(b, a_2, \ldots, a_n)}{\det(a_1, a_2, \ldots, a_n)} \\
>     & \vdots \\
>     x_n & = \frac{\det(a_1, \ldots, a_{n-1}, b)}{\det(a_1, \ldots, a_{n-1}, a_n)}
> \end{aligned}
> $

2. $\det( a_1, \ldots, a_n) = 0$

-   **ukÅ‚ad sprzeczny** (nie ma rozwiÄ…zaÅ„), gdy choÄ‡ jeden wyznacznik we wzorach Cramera zawierajÄ…cy $b$ jest rÃ³Å¼ny od zera

-   **ukÅ‚ad nieoznaczony** (ma wiÄ™cej niÅ¼ jedno rozwiÄ…zanie) lub sprzeczny, gdy wszystkie wyznaczniki we wzorach Cramera zawierajÄ…ce $b$ sÄ… rÃ³wne zeru.

> **WzÃ³r Cramera dla macierzy kwadratowej drugiego stopnia**
>
> $
> \begin{aligned}
>     A \mid B = \begin{cases}
>         ax + by = e \\
>         cx + dy = f
>     \end{cases}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     \begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}e \\ f\end{bmatrix}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     x = \frac{\begin{vmatrix}e & b \\ f & d\end{vmatrix}}{\begin{vmatrix}a & b \\ c & d\end{vmatrix}} = \frac{e \cdot d - b \cdot f}{a \cdot d - b \cdot c}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     y = \frac{\begin{vmatrix}a & e \\ c & f\end{vmatrix}}{\begin{vmatrix}a & b \\ c & d\end{vmatrix}} = \frac{a \cdot f - e \cdot c}{a \cdot d - b \cdot c}
> \end{aligned}
> $
>
> **PrzykÅ‚adowe rozwiÄ…zanie metodÄ… Cramera**
>
> $
> \begin{aligned}
>     A \mid B = \begin{cases}
>         2x + 5y + 3z = 5 \\
>         4x + 2y + 5z = 4 \\
>         3x + 8y + 4z = 9
>     \end{cases}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     \det{A} & = \begin{vmatrix}2 & 5 & 3 \\ 4 & 2 & 5 \\ 3 & 8 & 4\end{vmatrix} = 16 + 96 + 75 - 18 - 80 - 80 = 9 \\
>     \\
>     \det{A_x} & = \begin{vmatrix}5 & 5 & 3 \\ 4 & 2 & 5 \\ 9 & 8 & 4\end{vmatrix} = 40 + 96 + 225 - 54 - 200 - 80 = 27 \\
>     \det{A_y} & = \begin{vmatrix}2 & 5 & 3 \\ 4 & 4 & 5 \\ 3 & 9 & 4\end{vmatrix} = 32 + 108 + 75 - 36 - 90 - 80 = 9 \\
>     \det{A_z} & = \begin{vmatrix}2 & 5 & 5 \\ 4 & 2 & 4 \\ 3 & 8 & 9\end{vmatrix} = 36 + 160 + 60 - 30 - 64 - 180 = -18
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     x & = \frac{\det{A_x}}{\det{A}} & y & = \frac{\det{A_y}}{\det{A}} & z & = \frac{\det{A_z}}{\det{A}} \\
>     \\
>     x & = \frac{27}{9} & y & = \frac{9}{9} & z & = \frac{-18}{9} \\
>     x & = 3 & y & = 1 & z & = -2
> \end{aligned}
> $

##### ReguÅ‚a Sarrusa

Jest to praktyczny sposÃ³b obliczania wyznacznika trzeciego stopnia, gdzie skorzystanie z rozwiniÄ™cia Laplace'a moÅ¼e byÄ‡ niewygodne.

> **WzÃ³r wyznacznika Sarrusa**
>
> $
> \begin{aligned}
>     A = \begin{bmatrix}
>         a_{11} & a_{12} & a_{13} \\
>         a_{21} & a_{22} & a_{23} \\
>         a_{31} & a_{32} & a_{33}
>     \end{bmatrix}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     \det{A} = \begin{vmatrix}
>         a_{11} & a_{12} & a_{13} \\
>         a_{21} & a_{22} & a_{23} \\
>         a_{31} & a_{32} & a_{33}
>     \end{vmatrix} = a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{21}a_{12}a_{33}-a_{11}a_{32}a_{23}-a_{31}a_{22}a_{13}
> \end{aligned}
> $

##### RozwiniÄ™cie Laplace'a

WzÃ³r rekurencyjny okreÅ›lajÄ…cy wyznacznik n-tego stopnia macierzy kwadratowej o wymiarach $n \times n$.

> **WzÃ³r rozwiniÄ™cia Laplace'a**
>
> -   wzglÄ™dem wiersza
>
>     $\begin{aligned}\det{A} = \sum_{i = 1}^{n}{(-1)^{i + j} \cdot a_{ij} \cdot A_{ij}}\end{aligned}$
>
> -   wzglÄ™dem kolumny
>
>     $\begin{aligned}\det{A} = \sum_{j = 1}^{n}{(-1)^{i + j} \cdot a_{ij} \cdot A_{ij}}\end{aligned}$

Przy obliczaniu wyznacznika korzysta siÄ™ z twierdzenia Laplace'a tak dÅ‚ugo, aÅ¼ uzyska siÄ™ macierze, ktÃ³rych wyznaczniki moÅ¼na obliczyÄ‡ wprost (drugiego, trzeciego stopnia), gdzie $a_{ij}$ jest elementem macierzy w $i$-tym wierszu i $j$-tej kolumnie, a $A_{ij}$ jest dopeÅ‚nieniem algebraicznym elementu $a_{ij}.$

> **PrzykÅ‚adowe obliczenie wzglÄ™dem pierwszego wiersza**
>
> $
> A =
> \begin{bmatrix}
>     1 & 2 & 3 \\
>     4 & 5 & 6 \\
>     7 & 8 & 9
> \end{bmatrix}
> $
>
> $
> \begin{aligned}
>     \det{A} & = (-1)^{1 + 1} \cdot 1 \cdot \begin{vmatrix} 5 & 6 \\ 8 & 9 \end{vmatrix} + (-1)^{1 + 2} \cdot 2 \cdot \begin{vmatrix} 4 & 6 \\ 7 & 9 \end{vmatrix} + (-1)^{1 + 3} \cdot 3 \cdot \begin{vmatrix} 4 & 5 \\ 7 & 8 \end{vmatrix} \\
>     \det{A} & = 1 \cdot 1 \cdot (-3) - 1 \cdot 2 \cdot (-6) + 1 \cdot 3 \cdot (-3) = -3 + 12 - 9 = 0
> \end{aligned}
> $

### Twierdzenie Kroneckera-Capelliego

Twierdzenie algebry liniowej dajÄ…ce kryterium istnienia rozwiÄ…zaÅ„ ukÅ‚adu rÃ³wnaÅ„ liniowych i umoÅ¼liwiajÄ…ce ich klasyfikacjÄ™. UkÅ‚ad rÃ³wnaÅ„ liniowych $AX = B$ ma rozwiÄ…zanie wtedy i tylko wtedy, gdy rzÄ…d $r$ macierzy (najwiÄ™kszy niezerowy minor) $A$ jest rÃ³wny rzÄ™dowi macierzy rozszerzonej $A \mid B$ tego ukÅ‚adu.

1. $rank(A) \ne rank(A \mid B)$

-   ukÅ‚ad **sprzeczny**, wiÄ™c nie ma rozwiÄ…zaÅ„

2. $rank(A) = rank(A \mid B)$

-   ukÅ‚ad **oznaczony**, wiÄ™c ma dokÅ‚adnie jedno rozwiÄ…zanie
-   ukÅ‚ad **nieoznaczony**, gdy $rank(A) = rank(A \mid B) < n$, gdzie $n$ to liczba niewiadomych, wiÄ™c ma nieskoÅ„czenie wiele rozwiÄ…zaÅ„ zaleÅ¼nych od $n - r$ parametrÃ³w

## WartoÅ›ci wÅ‚asne macierzy i ich zastosowanie w informatyce

LiczbÄ™ zespolonÄ… $\lambda$ nazywamy wartoÅ›ciÄ… wÅ‚asnÄ… macierzy kwadratowej $A$, jeÅ¼eli istnieje niezerowy wektor $v$ taki, Å¼e $A v = \lambda v$. Wektor wÅ‚asny przeksztaÅ‚cenia macierzy, to taki niezerowy wektor, ktÃ³ry nie zmienia kierunku po przeksztaÅ‚ceniu. JeÅ›li $v$ jest wektorem wÅ‚asnym macierzy $A$, to $Av$ jest proporcjonalne do $v$, gdzie proporcjonalnoÅ›Ä‡ jest okreÅ›lana przez wartoÅ›Ä‡ wÅ‚asnÄ….

> **PrzykÅ‚adowe wyznaczenie wartoÅ›ci i wektorÃ³w wÅ‚asnych macierzy**
>
> $
> A =
> \begin{bmatrix}
>     6 & 3 \\
>     2 & 5
> \end{bmatrix}
> $
>
> $
> \begin{aligned}
>     A_\lambda & =
>     \begin{bmatrix}
>         6 & 3 \\
>         2 & 5
>     \end{bmatrix}
>     - \lambda \cdot
>     \begin{bmatrix}
>         1 & 0 \\
>         0 & 1
>     \end{bmatrix} \\
>     A_\lambda & =
>     \begin{bmatrix}
>         6 & 3 \\
>         2 & 5
>     \end{bmatrix}
>     -
>     \begin{bmatrix}
>         \lambda & 0 \\
>         0 & \lambda
>     \end{bmatrix} \\
>     A_\lambda & =
>     \begin{bmatrix}
>         6 - \lambda & 3 \\
>         2 & 5 - \lambda
>     \end{bmatrix}
> \end{aligned}
> $
>
> $
> \det{A_\lambda} = (6 âˆ’ \lambda)(5 âˆ’ \lambda) - 2 \cdot 3 = \lambda^2 âˆ’ 11\lambda + 24
> $
>
> $
> \begin{aligned}
>     \Delta & = 25 \\
>     \lambda_1 & = \frac{11 - 5}{2} = 3 \\
>     \lambda_2 & = \frac{11 + 5}{2} = 8
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     A_{\lambda_1} & =
>     \begin{bmatrix}
>         3 & 3 \\
>         2 & 2
>     \end{bmatrix} \\
>     A_{\lambda_2} & =
>     \begin{bmatrix}
>         -2 & 3 \\
>         2 & -3
>     \end{bmatrix}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     v_{\lambda_1} & =
>     \begin{cases}
>         3x + 3y = 0 \\
>         2x + 2y = 0
>     \end{cases} \\
>     v_{\lambda_1} & = \begin{bmatrix}1 \\ -1\end{bmatrix}
> \end{aligned}
> $
>
> $
> \begin{aligned}
>     v_{\lambda_2} & =
>     \begin{cases}
>         -2x + 3y = 0 \\
>         2x + -3y = 0
>     \end{cases} \\
>     v_{\lambda_2} & = \begin{bmatrix}3 \\ 2\end{bmatrix}
> \end{aligned}
> $

### Zastosowanie wartoÅ›ci wÅ‚asnych w informatyce

#### Grafika komputerowa

W grafice komputerowej wartoÅ›ci wÅ‚asne mogÄ… byÄ‡ uÅ¼ywane do przeksztaÅ‚ceÅ„ macierzy, takich jak obracanie, skalowanie i przesuwanie obiektÃ³w w trÃ³jwymiarowej przestrzeni. PomagajÄ… rÃ³wnieÅ¼ w analizie i syntezie obrazÃ³w, na przykÅ‚ad w analizie tekstur i kompresji obrazÃ³w.

#### Uczenie maszynowe

W uczeniu maszynowym, zwÅ‚aszcza w analizie spektralnej i analizie grafÃ³w, wartoÅ›ci wÅ‚asne macierzy sÄ… uÅ¼ywane do wykrywania struktur w danych, redukcji wymiarowoÅ›ci, grupowania oraz rozpoznawania wzorcÃ³w. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm spektralny do analizy grafÃ³w.

#### Kompresja danych

WartoÅ›ci wÅ‚asne mogÄ… byÄ‡ wykorzystywane w kompresji danych, na przykÅ‚ad w algorytmie kompresji obrazÃ³w JPEG. Algorytm ten polega na dekompozycji obrazu na komponenty przeksztaÅ‚cone za pomocÄ… wartoÅ›ci wÅ‚asnych macierzy kowariancji.

## Grafy i ich typy, metody reprezentacji grafÃ³w

Graf to zbiÃ³r wierzchoÅ‚kÃ³w, ktÃ³re mogÄ… byÄ‡ poÅ‚Ä…czone krawÄ™dziami w taki sposÃ³b, Å¼e kaÅ¼da krawÄ™dÅº koÅ„czy siÄ™ i zaczyna w ktÃ³rymÅ› z wierzchoÅ‚kÃ³w. W matematyce graf $G = (V, E)$ zdefiniowany jest jako uporzÄ…dkowana para zbiorÃ³w wierzchoÅ‚kÃ³w oraz krawÄ™dzi, gdzie $V$ to zbiÃ³r wierzchoÅ‚kÃ³w, a $E$ to zbiÃ³r krawÄ™dzi.

### Rodzaje grafÃ³w

#### Graf nieskierowany

```mermaid
flowchart TB
    1(( )) --- 2(( ))
    2(( )) --- 3(( ))
    3(( )) --- 1(( ))
    2(( )) --- 4(( ))
```

-   kaÅ¼da krawÄ™dÅº $e = \{v, w\}$ ze zbioru $E$ to nieuporzÄ…dkowana para wierzchoÅ‚kÃ³w ze zbioru $V$, zwanych koÅ„cami krawÄ™dzi $e$
-   dla krawÄ™dzi $e = \{v, w\} \in E$
    -   krawÄ™dÅº $e$ Å‚Ä…czy wierzchoÅ‚ki $v$ i $w$
    -   wierzchoÅ‚ki $v$ i $w$ sÄ… sÄ…siednie w grafie
    -   krawÄ™dÅº $e$ jest incydentna z wierzchoÅ‚kami $v$ i $w$
-   reprezentuje symetryczna relacjÄ™ binarnÄ… na zbiorze wierzchoÅ‚kÃ³w

#### Graf skierowany

```mermaid
flowchart TB
    1(( )) --> 2(( ))
    2(( )) --> 3(( ))
    3(( )) --> 1(( ))
    2(( )) --> 4(( ))
```

-   kaÅ¼da krawÄ™dÅº $e = \{v, w\}$ ze zbioru $E$ to uporzÄ…dkowana para wierzchoÅ‚kÃ³w ze zbioru $V$, zwanych poczÄ…tkiem i koÅ„cem krawÄ™dzi $e$
-   dla krawÄ™dzi $e = \{v, w\} \in E$
    -   krawÄ™dÅº $e$ biegnie od $v$ do $w$
    -   krawÄ™dzie skierowane nazywane sÄ…Â Å‚ukami
    -   krawÄ™dÅº $e$ jest incydentna z wierzchoÅ‚kami $v$ i $w$
-   reprezentuje dowolnÄ… relacjÄ™ binarnÄ… na zbiorze wierzchoÅ‚kÃ³w

#### Graf z wagami

```mermaid
flowchart TB
    1(( )) ---|2| 2(( ))
    2(( )) ---|1| 3(( ))
    3(( )) ---|3| 1(( ))
    2(( )) ---|7| 4(( ))
```

Graf, w ktÃ³rym kaÅ¼da krawÄ™dÅº ma przypisanÄ… wagÄ™ lub koszt, ktÃ³ry reprezentuje pewnÄ… miarÄ™ odlegÅ‚oÅ›ci lub kosztu przejÅ›cia miÄ™dzy wierzchoÅ‚kami.

#### Graf prosty

```mermaid
flowchart LR
    1(( )) --- 2(( ))
    2(( )) --- 3(( ))
    3(( )) --- 1(( ))
```

Graf bez pÄ™tli wÅ‚asnych i krawÄ™dzi wielokrotnych.

#### Graf peÅ‚ny

```mermaid
flowchart LR
    1(( )) --- 2(( ))
    1(( )) --- 3(( ))
    2(( )) --- 4(( ))
    2(( )) --- 3(( ))
    3(( )) --- 4(( ))
    4(( )) --- 1(( ))
```

DopeÅ‚nienie grafu pustego (posiada same wierzchoÅ‚ki bez krawÄ™dzi), gdzie kaÅ¼dy wierzchoÅ‚ek jest poÅ‚Ä…czony z kaÅ¼dym. Graf zawierajÄ…cy tylko jeden wierzchoÅ‚ek jest peÅ‚ny.

#### Graf cykliczny

```mermaid
flowchart LR
    1(( )) --- 2(( )) & 3(( )) --- 4(( ))
```

Zawiera przynajmniej jeden cykl, czyli sekwencjÄ™ wierzchoÅ‚kÃ³w, w ktÃ³rej moÅ¼na wrÃ³ciÄ‡ do punktu poczÄ…tkowego, przechodzÄ…c przez krawÄ™dzie. W przypadku grafÃ³w nieskierowanych spÃ³jnych grafy acykliczne sÄ… rÃ³wnowaÅ¼ne drzewom, a niespÃ³jne lasom.

#### Graf spÃ³jny

```mermaid
flowchart TB
    1(( )) --- 2(( ))
    1(( )) --- 3(( ))
    2(( )) --- 3(( ))
    4(( )) --- 2(( ))
```

Istnieje Å›cieÅ¼ka miÄ™dzy kaÅ¼dÄ… parÄ… wierzchoÅ‚kÃ³w.

### Metody reprezentacji grafÃ³w

```mermaid
flowchart LR
    2((2)) ---|1| 1((1))
    2((2)) ---|2| 3((3))
    4((4)) ---|3| 3((3))
    5((5)) ---|4| 1((1))
    5((5)) ---|5| 2((2))
    5((5)) ---|6| 4((4))
    6((6)) ---|7| 4((4))
```

#### Macierz sÄ…siedztwa

$
A = \begin{bmatrix}
    0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 0
\end{bmatrix}
$

Graf jest reprezentowany jako macierz, gdzie $a_ij$ oznacza liczbÄ™ krawÄ™dzi miÄ™dzy wierzchoÅ‚kami $i$ i $j$. W przypadku grafÃ³w waÅ¼onych, komÃ³rki zawierajÄ…ce wagi krawÄ™dzi.

#### Macierz incydencji

$
A = \begin{bmatrix}
    1 & 1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 1 & 1 \\
    1 & 0 & 0 & 0 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}
$

Graf jest reprezentowany jako macierz, gdzie wiersze reprezentujÄ… wierzchoÅ‚ki, a kolumny reprezentujÄ… krawÄ™dzie. W komÃ³rkach macierzy jest 1, jeÅ›li dany wierzchoÅ‚ek jest incydentny z danÄ… krawÄ™dziÄ…, -1, jeÅ›li krawÄ™dÅº wychodzi z wierzchoÅ‚ka, a 0 w przeciwnym razie.

#### Listy sÄ…siedztwa

$
\begin{aligned}
    1 & \rightarrow 2, 5 \\
    2 & \rightarrow 1, 3, 5 \\
    3 & \rightarrow 2, 4 \\
    4 & \rightarrow 3, 5, 6 \\
    5 & \rightarrow 1, 2, 4 \\
    6 & \rightarrow 4
\end{aligned}
$

Graf jest reprezentowany jako lista dla kaÅ¼dego wierzchoÅ‚ka, zawierajÄ…ca informacje o sÄ…siednich wierzchoÅ‚kach lub krawÄ™dziach.

## Relacje binarne, wÅ‚asnoÅ›ci i metody reprezentacji

Dowolny podzbiÃ³r produktu kartezjaÅ„skiego $X \times Y$ nazywamy relacjÄ… dwuargumentowÄ… (binarnÄ…) w $X \times Y$.

### WÅ‚asnoÅ›ci relacji

-   **Relacja zwrotna** ($\forall{x \in X}: x R x$)

    Relacja, w ktÃ³rej kaÅ¼dy element jest w relacji sam ze sobÄ….

    > Dla dowolnej wartoÅ›ci $x$ zachodzi $x = x$.

-   **Relacja przeciwzwrotna** ($\forall{x \in X}: \neg x R x$)

    Relacja, w ktÃ³rej Å¼aden element nie jest w relacji sam ze sobÄ….

    > Polska graniczy z Niemcami, ale Polska nie graniczy z PolskÄ….

-   **Relacja symetryczna** ($\forall{x, y \in X}: x R y \implies y R x$)

    Relacja, w ktÃ³rej jeÅ›li para $(x, y)$ naleÅ¼y do relacji, to para $(y, x)$ takÅ¼e naleÅ¼y do relacji. PrzykÅ‚adem jest relacja "jest znajomym" w kontekÅ›cie sieci spoÅ‚ecznoÅ›ciowych.

    > Jan jest znajomym Anny, wiÄ™c Anna jest znajomÄ… Jana.

-   **Relacja asymetryczna** ($\forall{x, y \in X}: x R y \implies \neg y R x$)

    Relacja, w ktÃ³rej dla kaÅ¼dej pary $(x, y)$, jeÅ›li para $(x, y)$ naleÅ¼y do relacji, to para $(y, x)$ nie moÅ¼e naleÅ¼eÄ‡ do tej samej relacji, chyba Å¼e $x$ i $y$ sÄ… tym samym elementem (czyli $x = y$).

    > 4 jest kwadratem liczby 2, ale 2 nie jest kwadratem liczby 4.

-   **Relacja antysymetryczna** ($\forall{x, y \in X}: x R y \land y R x \implies x = y$)

    Relacja, w ktÃ³rej jeÅ›li para $(x, y)$ naleÅ¼y do relacji i para $(x, y)$ takÅ¼e naleÅ¼y do relacji, to $x$ musi byÄ‡ rÃ³wne $y$. PrzykÅ‚adem moÅ¼e byÄ‡ relacja "jest rodzicem" w genealogii.

    > 2 jest dzielnikiem liczby 4, ale 4 nie jest dzielnikiem liczby 2.

-   **Relacja przechodnia** ($\forall{x, y, z \in X}: x R y \land y R z \implies x R z$)

    Relacja, w ktÃ³rej jeÅ›li para $(x, y)$ i para $(y, z)$ naleÅ¼Ä… do relacji, to para $(x, z)$ takÅ¼e naleÅ¼y do relacji.

    > Jan jest przodkiem Anny, a Anna jest przodkiem Stefana, wiÄ™c Jan jest przodkiem Stefana.

-   **Relacja spÃ³jna** ($\forall{x, y \in X}: x R y \lor y R x$)

    Relacja, w ktÃ³rej istnieje Å›cieÅ¼ka miÄ™dzy dowolnymi dwoma elementami zbioru.

    > Albo $x$ jest mniejszy od $y$, albo $y$ jest mniejszy od $x$.

-   **Relacja rÃ³wnowaÅ¼noÅ›ci**

    Relacja, ktÃ³ra jest zwrotna, symetryczna i przechodnia.

-   **Relacja porzÄ…dku czÄ™Å›ciowego**

    Relacja, ktÃ³ra jest zwrotna, asymetryczna i przechodnia.

-   **Relacja porzÄ…dku liniowego**

    Relacja, ktÃ³ra jest zwrotna, asymetryczna, przechodnia i spÃ³jna.

### Metody reprezentacji relacji binarnych

#### Tabela

|     |  ğŸ‡µğŸ‡±   |  ğŸ‡¨ğŸ‡¿   |  ğŸ‡¸ğŸ‡°   |  ğŸ‡ºğŸ‡¦   |
| :-: | :---: | :---: | :---: | :---: |
| ğŸ‡µğŸ‡±  |   0   | **1** | **1** | **1** |
| ğŸ‡¨ğŸ‡¿  | **1** |   0   | **1** |   0   |
| ğŸ‡¸ğŸ‡°  | **1** | **1** |   0   | **1** |
| ğŸ‡ºğŸ‡¦  | **1** |   0   | **1** |   0   |

#### Graf

```mermaid
flowchart LR
    subgraph X
    PL_X(("ğŸ‡µğŸ‡±"))
    CZ_X(("ğŸ‡¨ğŸ‡¿"))
    SK_X(("ğŸ‡¸ğŸ‡°"))
    UA_X(("ğŸ‡ºğŸ‡¦"))
    end
    subgraph Y
    PL_Y(("ğŸ‡µğŸ‡±"))
    CZ_Y(("ğŸ‡¨ğŸ‡¿"))
    SK_Y(("ğŸ‡¸ğŸ‡°"))
    UA_Y(("ğŸ‡ºğŸ‡¦"))
    end

PL_X --> CZ_Y
PL_X --> SK_Y
PL_X --> UA_Y
CZ_X--> PL_Y
CZ_X--> SK_Y
SK_X --> PL_Y
SK_X --> CZ_Y
SK_X --> UA_Y
UA_X --> PL_Y
UA_X --> CZ_Y
UA_X --> SK_Y
```

## Zasada indukcji matematycznej

Indukcja matematyczna to technika dowodzenia stosowana gÅ‚Ã³wnie w matematyce do pokazywania, Å¼e pewne twierdzenie jest prawdziwe dla wszystkich liczb naturalnych. DowÃ³d polega na tym, Å¼e pewne twierdzenie $P(n)$ jest prawdziwe dla kaÅ¼dej liczby naturalnej $n$

**Algorytm indukcji matematycznej**

1. **Baza indukcyjna:**
   Pokazanie, Å¼e twierdzenie jest prawdziwe dla pewnej poczÄ…tkowej liczby naturalnej, zazwyczaj dla $n = 1$. Czyli udowodnienie $P(1)$.

2. **Krok indukcyjny:** ZakÅ‚adajÄ…c, Å¼e twierdzenie jest prawdziwe dla pewnej dowolnej, ale staÅ‚ej liczby $k$ (to jest tzw. zaÅ‚oÅ¼enie indukcyjne), naleÅ¼y udowodniÄ‡, Å¼e bÄ™dzie to rÃ³wnieÅ¼ prawdziwe dla liczby $k + 1$. Czyli, zakÅ‚adajÄ…c $P(k)$, udowodnienie $P(k + 1)$.

JeÅ›li oba powyÅ¼sze kroki sÄ… speÅ‚nione, moÅ¼emy stwierdziÄ‡, Å¼e twierdzenie $P(n)$ jest prawdziwe dla kaÅ¼dej liczby naturalnej $n$.

> **PrzykÅ‚adowy dowÃ³d, Å¼e suma pierwszych $n$ liczb naturalnych wynosi $\frac{n(n + 1)}{2}$**
>
> 1.  Dla $n = 1$
>
>     $\begin{aligned}1 = \frac{1(1 + 1)}{2}\end{aligned}$
>
> 2.  ZaÅ‚oÅ¼enie, Å¼e twierdzenie jest prawdziwe dla pewnego $k$
>
>     Czyli $1 + 2 + \ldots + k = \frac{k(k + 1)}{2}$, wiÄ™c naleÅ¼y wykazaÄ‡, Å¼e to jest prawdziwe dla $k + 1$.
>
>     $\begin{aligned}1 + 2 + \ldots + k + (k + 1) = \frac{k(k + 1)}{2} + (k + 1)\end{aligned}$
>
>     $\begin{aligned}\frac{k(k + 1) + 2(k + 1)}{2} = \frac{(k + 1)(k + 2)}{2}\end{aligned}$
>
>     Z powyÅ¼szego dowodu wynika, Å¼e twierdzenie jest prawdziwe dla kaÅ¼dej liczby naturalnej.

## Twierdzenie Bayesa

Twierdzenie teorii prawdopodobieÅ„stwa, wiÄ…Å¼Ä…ce prawdopodobieÅ„stwa warunkowe dwÃ³ch zdarzeÅ„ warunkujÄ…cych siÄ™ nawzajem.

> **WzÃ³r Bayesa**
>
> $\begin{aligned}P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\end{aligned}$

$A$ i $B$ sÄ… zdarzeniami oraz $P(B) \gt 0$.

$P(A)$ nazywane jest "wstÄ™pnym prawdopodobieÅ„stwem" lub "a priori". $P(B \mid A)$ to prawdopodobieÅ„stwo uzyskania obserwacji $B$, gdy zakÅ‚adamy, Å¼e hipoteza $A$ jest prawdziwa. $P(B)$ to caÅ‚kowite prawdopodobieÅ„stwo obserwacji $B$. $P(A \mid B)$ to prawdopodobieÅ„stwo warunkowe zdarzenia nazywane "posterior" lub "a posteriori" i reprezentuje zaktualizowane prawdopodobieÅ„stwo hipotezy $A$ po uwzglÄ™dnieniu obserwacji $B$.

## Testowanie hipotez statystycznych

Testowanie hipotez statystycznych to formalny proces, dziÄ™ki ktÃ³remu moÅ¼na wnioskowaÄ‡ na temat populacji na podstawie prÃ³bki danych. GÅ‚Ã³wnym celem jest ocena, czy istnieje wystarczajÄ…co duÅ¼o dowodÃ³w w naszych danych do odrzucenia pewnej zaÅ‚oÅ¼onej hipotezy, zwanej hipotezÄ… zerowÄ…, na korzyÅ›Ä‡ alternatywnej hipotezy.

HipotezÄ… nazywamy dowolne przypuszczenie, ktÃ³re chcemy zweryfikowaÄ‡. JeÅ¼eli przypuszczenie dotyczy rozkÅ‚adu prawdopodobieÅ„stwa to mamy do czynienia z hipotezÄ… statystycznÄ…. TestowaÄ‡ moÅ¼na zarÃ³wno typ rozkÅ‚adu jak i jego parametry.

BadajÄ…c hipotezÄ™ moÅ¼emy popeÅ‚niÄ‡ dwa rodzaje bÅ‚Ä™dÃ³w:

1. **BÅ‚Ä…d pierwszego rodzaju**

    Odrzucamy hipotezÄ™ zerowÄ…, mimo Å¼e jest ona prawdziwa.

2. **BÅ‚Ä…d drugiego rodzaju**

    Nie odrzucamy hipotezy zerowej, mimo, Å¼e nie jest prawdziwa.

### Kroki weryfikacji hipotez statystycznych

1. **SformuÅ‚owanie hipotez**

    Hipoteza zerowa i alternatywna majÄ… siÄ™ wzajemnie wykluczaÄ‡. DÄ…Å¼y siÄ™ do udowodnienia hipotezy alternatywnej, co pozwoli na odrzucenie hipotezy zerowej. Odrzucenie hipotezy alternatywnej nie oznacza, Å¼e hipoteza zerowa jest prawdziwa ani faÅ‚szywa.

    - $H_0$ (hipoteza zerowa)

        ZaÅ‚oÅ¼enie poczÄ…tkowe, ktÃ³re chcemy przetestowaÄ‡. PrawdziwoÅ›Ä‡ jest wÄ…tpliwa i jest testowana celem ewentualnego odrzucenia.

    - $H_1$ (hipoteza alternatywna)

        Jest hipotezÄ…, ktÃ³ra bÄ™dzie przyjÄ™ta, jeÅ›li odrzucimy hipotezÄ™ zerowÄ…

2. **WybÃ³r poziomu istotnoÅ›ci** ($\alpha$)

    Poziom istotnoÅ›ci, czÄ™sto okreÅ›lany jako $\alpha$, to prawdopodobieÅ„stwo odrzucenia hipotezy zerowej, gdy jest ona prawdziwa.

3. **WybÃ³r odpowiedniej statystyki testowej**

    Statystyka, na ktÃ³rej podstawie orzekamy prawdziwoÅ›Ä‡ $H_1$, oznaczana wielkÄ… literÄ… $Z$ (statystyka to kwantyl danego rozkÅ‚adu. Z karty wzorÃ³w patrzymy na odpowiedni kwantyl, zaleÅ¼ny od poziomu istotnoÅ›ci, i sprawdzamy, czy nasza statystyka testowa jest od niego wiÄ™ksza. To znaczy, Å¼e naleÅ¼y ona do zbioru krytycznego).

    WybÃ³r testu zaleÅ¼y od charakterystyki danych (np. czy dane sÄ… normalnie rozÅ‚oÅ¼one), rodzaju mierzonej zmiennej oraz iloÅ›ci grup, ktÃ³re sÄ… porÃ³wnywane.

4. **OkreÅ›lenie zbioru krytycznego**

    Obszar krytyczny znajduje siÄ™ zawsze na kraÅ„cach rozkÅ‚adu. JeÅ¼eli obliczona przez nas wartoÅ›Ä‡ statystyki testowej znajdzie siÄ™ w tym obszarze, to weryfikowanÄ… przez nas hipotezÄ™ $H_0$ odrzucamy. WielkoÅ›Ä‡ obszaru krytycznego wyznacza dowolnie maÅ‚y poziom istotnoÅ›ci $\alpha$, natomiast jego poÅ‚oÅ¼enie okreÅ›lane jest przez hipotezÄ™ alternatywnÄ….

5. **Decyzja**

    JeÅ›li wartoÅ›Ä‡ statystyki testowej jest wystarczajÄ…co ekstremalna (zaleÅ¼nie od przyjÄ™tego poziomu istotnoÅ›ci i rodzaju testu), odrzucamy hipotezÄ™ zerowÄ… na korzyÅ›Ä‡ hipotezy alternatywnej. MoÅ¼emy rÃ³wnieÅ¼ porÃ³wnaÄ‡ obliczonÄ… wartoÅ›Ä‡ p (p-wartoÅ›Ä‡) z wybranym poziomem istotnoÅ›ci ($\alpha$). JeÅ›li p-wartoÅ›Ä‡ jest mniejsza niÅ¼ $\alpha$, odrzucamy hipotezÄ™ zerowÄ….

6. **Wnioskowanie**

    Odrzucenie $H_0$ nie oznacza, Å¼e jest ona faÅ‚szywa, ani Å¼e $H_1$ jest prawdziwa. Oznacza to jedynie, Å¼e mamy wystarczajÄ…co duÅ¼o dowodÃ³w, aby odrzuciÄ‡ $H_0$ przy danym poziomie istotnoÅ›ci.

## Wyznaczanie przedziaÅ‚Ã³w ufnoÅ›ci

Wyznaczanie przedziaÅ‚Ã³w ufnoÅ›ci polega na okreÅ›leniu zakresu wartoÅ›ci, w ktÃ³rym z okreÅ›lonym prawdopodobieÅ„stwem (poziomem ufnoÅ›ci) spodziewamy siÄ™ znaleÅºÄ‡ nieznany parametr populacji. PrzedziaÅ‚y ufnoÅ›ci pozwalajÄ… nam okreÅ›liÄ‡ zakres wartoÅ›ci, w jakich znajduje siÄ™ dany parametr na $1 - \alpha$, gdzie $\alpha$ okreÅ›la poziom ufnoÅ›ci.

Dla danego poziomu ufnoÅ›ci $\alpha$, przedziaÅ‚ ufnoÅ›ci dla parametru $\theta$ jest zakresem wartoÅ›ci, w ktÃ³rym prawdopodobieÅ„stwo zawarcia prawdziwej wartoÅ›ci $\theta$ wynosi $1 - \alpha$.

### PrzedziaÅ‚ ufnoÅ›ci dla Å›redniej

#### Znane odchylenie standardowe

Cecha ma w populacji rozkÅ‚ad normalny $N(\mu, \sigma)$ przy czym odchylenie standardowe $\sigma$ jest znane.

> **PrzedziaÅ‚ ufnoÅ›ci dla parametru $\mu$ rozkÅ‚adu normalnego**
>
> $\begin{aligned}P\left(\overline{X} - u_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu < \overline{X} + u_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \right) = 1 - \alpha\end{aligned}$
>
> -   $n$ â€“ liczebnoÅ›Ä‡ prÃ³by losowej
> -   $\overline{X}$ â€“ Å›rednia z prÃ³by losowej
> -   $\sigma$ â€“ odchylenie standardowe populacji
> -   $u_{1 - \frac{\alpha}{2}}=-u_\frac{\alpha}{2}$ to kwantyl rzÄ™dÃ³w $1 - \frac{\alpha}{2}$ rozkÅ‚adu $N(0, 1)$

#### Nieznane odchylenie standardowe

Cecha ma w populacji rozkÅ‚ad normalny $N(\mu, \sigma)$, przy czym odchylenie standardowe $\sigma$ jest nieznane.

> **PrzedziaÅ‚ ufnoÅ›ci dla parametru $\mu$ rozkÅ‚adu normalnego**
>
> $\begin{aligned}P\left(\overline{X} - t_{1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n}} < \mu < \overline{X} + t_{1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n}} \right) = 1 - \alpha\end{aligned}$
>
> -   $n$ â€“ liczebnoÅ›Ä‡ prÃ³by losowej
> -   $\overline{X}$ â€“ Å›rednia z prÃ³by losowej
> -   $S$ â€“ odchylenie standardowe z prÃ³by
> -   $t_{1- \frac{\alpha}{2}}$ to kwantyl rzÄ™du $1-\frac{\alpha}{2}$ rozkÅ‚adu Studenta z $n - 1$ stopniami swobody

Zwykle stosuje siÄ™ ten wzÃ³r dla maÅ‚ej prÃ³by $(n < 30)$. Tak naprawdÄ™ dziaÅ‚a on dla kaÅ¼dej wielkoÅ›ci prÃ³by, jednak dla duÅ¼ych prÃ³b moÅ¼na przybliÅ¼yÄ‡ rozkÅ‚ad t-Studenta rozkÅ‚adem normalnym, co jest Å‚atwiejsze do wyliczenia a dajÄ…ce niemal takie same wartoÅ›ci.

#### Nieznane odchylenie standardowe â€“ duÅ¼a prÃ³ba (n > 30)

Cecha ma w populacji rozkÅ‚ad normalny $N(\mu, \sigma)$, przy czym odchylenie standardowe $\sigma$ jest nieznane, a prÃ³ba jest duÅ¼a $(n > 30)$. Granica 30 jest czysto umowna, im $n$ jest wiÄ™ksze, tym wzÃ³r dokÅ‚adniejszy.

> **PrzedziaÅ‚ ufnoÅ›ci dla parametru $n$ rozkÅ‚adu normalnego**
>
> $\begin{aligned}P\left(\overline{X} - u_{1-\frac{\alpha}{2}} \frac{S}{\sqrt{n}} < \mu < \overline{X} + u_{1-\frac{\alpha}{2}} \frac{S}{\sqrt{n}} \right) = 1 - \alpha\end{aligned}$
>
> -   $n$ â€“ liczebnoÅ›Ä‡ prÃ³by losowej
> -   $\overline{X}$ â€“ Å›rednia z prÃ³by losowej
> -   $S$ â€“ odchylenie standardowe z prÃ³by
> -   $u_{1-\frac{\alpha}{2}}$ â€“ statystyka ze zmiennÄ… losowÄ… o rozkÅ‚adzie normalnym $N(0, 1)$

### PrzedziaÅ‚ ufnoÅ›ci dla wariancji

> **PrzedziaÅ‚ ufnoÅ›ci dla wariancji w populacji o rozkÅ‚adzie normalnym**
>
> $\begin{aligned}P\left(\frac{nS^2}{\chi^2_{1 - \frac{\alpha}{2}, n - 1}} < \sigma^2 < \frac{nS^2}{\chi^2_{\frac{\alpha}{2}, n - 1}} \right) = 1 - \alpha\end{aligned}$
>
> -   $n$ â€“ liczebnoÅ›Ä‡ prÃ³by losowej
> -   $S$ â€“ odchylenie standardowe z prÃ³by
> -   $\chi^2_{\frac{\alpha}{2}, n - 1}$ i $\chi^2_{1 - \frac{\alpha}{2}, n - 1}$ â€“ statystyki speÅ‚niajÄ…ce odpowiednio nierÃ³wnoÅ›ci:
>     -   $\begin{aligned}P\left(\chi^2 \geqslant \chi^2_{\frac{\alpha}{2}, n - 1} \right) = \frac{\alpha}{2}\end{aligned}$
>     -   $\begin{aligned}P\left(\chi^2 \geqslant \chi^2_{1 - \frac{\alpha}{2}, n - 1} \right) = 1 - \frac{\alpha}{2}\end{aligned}$

$\chi^2$ ma rozkÅ‚ad chi-kwadrat z $n - 1$ stopniami swobody.

Podobnie jak poprzednio, zwykle stosuje siÄ™ ten wzÃ³r dla maÅ‚ej prÃ³by $(n < 30)$, choÄ‡ rÃ³wnieÅ¼ dziaÅ‚a on dla kaÅ¼dej wielkoÅ›ci prÃ³by.

#### DuÅ¼a prÃ³ba (n > 30)

> **PrzedziaÅ‚ ufnoÅ›ci dla wariancji w populacji o rozkÅ‚adzie normalnym**
>
> $\begin{aligned}P\left(\frac{S}{1+ \frac{u_\alpha}{\sqrt{2n}}} < \sigma < \frac{S}{1 - \frac{u_\alpha}{\sqrt{2n}}} \right) = 1 - \alpha\end{aligned}$
>
> -   $n$ â€“ liczebnoÅ›Ä‡ prÃ³by losowej
> -   $S$ â€“ odchylenie standardowe z prÃ³by
> -   $u_\alpha$ â€“ statystyka speÅ‚niajÄ…ca warunek:
>     -   $\begin{aligned}P(- u_\alpha < U < u_\alpha) = 1 - \alpha\end{aligned}$

$U$ jest zmiennÄ… losowÄ… o rozkÅ‚adzie normalnym $N(0, 1)$.
